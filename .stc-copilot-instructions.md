# Goals

* Multiple model provider choices such as :  openai, ollama, gemini.


## Multiple model provider

This section provide samples of how we desire this model provider to be implemented.

* I am expecting that you will make the loading of provider from configuration and that each provider will have its own ways to implement the agent (ex. provider: openai : " from langchain_openai import ChatOpenAI,OpenAI"..., ollama: "from langchain_ollama import OllamaLLM" )


### 2 Provider Config example

* You dont have to follow that exactly, they are to teach you the structure of it:  <PROVIDER_NAME>://<MODEL>@[HOSTNAME]

```python
MY_MODEL_PROVIDER_EXAMPLE1 = (
    "google://gemini-1.5-pro-002"  # Note this value is overridden by the argparser
)

```

```python
MY_MODEL_PROVIDER_EXAMPLE2 = "ollama://llama3.1"  # Note this value is overridden by the argparser
```
```python
MY_MODEL_PROVIDER_EXAMPLE2 = "ollama://llama3.1@localhost"  # Note this value is overridden by the argparser
```
```python
MY_MODEL_PROVIDER_EXAMPLE2 = "ollama://llama3.1@myollamaserver.com"  # Note this value is overridden by the argparser
```

### Example class that load the provider from config

* Observe how they setup a generic "ProviderModel" that got created from :  <PROVIDER_NAME>://<MODEL>@[HOSTNAME]

```python

    def LoadModels(self, Models: list):
        for Model in Models:
            if Model in self.Clients:
                continue
            else:
                Provider, ProviderModel, ModelHost, ModelOptions = (
                    self.GetModelAndProvider(Model)
                )
                print(f"DEBUG: Loading Model {ProviderModel} from {Provider}@{ModelHost}")

                if Provider == "ollama":
                    # Get ollama models (only once)
                    self.ensure_package_is_installed("ollama")
                    import ollama

                    OllamaHost = ModelHost if ModelHost is not None else None

                    # Check if availabel via ollama.show(Model)
                    # check if the model is in the list of models
                    try:
                        ollama.Client(host=OllamaHost).show(ProviderModel)
                        pass
                    except Exception as e:
                        print(
                            f"Model {ProviderModel} not found in Ollama models. Downloading..."
                        )
                        OllamaDownloadStream = ollama.Client(host=OllamaHost).pull(
                            ProviderModel, stream=True
                        )
                        for chunk in OllamaDownloadStream:
                            if "completed" in chunk and "total" in chunk:
                                OllamaDownloadProgress = (
                                    chunk["completed"] / chunk["total"]
                                )
                                completedSize = chunk["completed"] / 1024**3
                                totalSize = chunk["total"] / 1024**3
                                print(
                                    f"Downloading {ProviderModel}: {OllamaDownloadProgress * 100:.2f}% ({completedSize:.3f}GB/{totalSize:.3f}GB)",
                                    end="\r",
                                )
                            else:
                                print(f"{chunk['status']} {ProviderModel}", end="\r")
                        print("\n\n\n")

                    self.Clients[Model] = ollama.Client(host=OllamaHost)
                    print(f"OLLAMA Host is '{OllamaHost}'")

                elif Provider == "google":
                    # Validate Google API Key
                    if (
                        not "GOOGLE_API_KEY" in os.environ
                        or os.environ["GOOGLE_API_KEY"] == ""
                    ):
                        raise Exception(
                            "GOOGLE_API_KEY not found in environment variables"
                        )
                    self.ensure_package_is_installed("google-generativeai")
                    import google.generativeai as genai

                    genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
                    self.Clients[Model] = genai.GenerativeModel(
                        model_name=ProviderModel
                    )

                elif Provider == "openai":
                    raise NotImplementedError("OpenAI API not supported")
                
                elif Provider == "myflowise":
                    self.Clients[Model] = {
                        "api_url": f"http://{ModelHost}/api/v1/prediction/{ProviderModel}"
                    }

                elif Provider == "openrouter":
                    if (
                        not "OPENROUTER_API_KEY" in os.environ
                        or os.environ["OPENROUTER_API_KEY"] == ""
                    ):
                        raise Exception(
                            "OPENROUTER_API_KEY not found in environment variables"
                        )
                    from Writer.Interface.OpenRouter import OpenRouter

                    self.Clients[Model] = OpenRouter(
                        api_key=os.environ["OPENROUTER_API_KEY"], model=ProviderModel
                    )

                elif Provider == "Anthropic":
                    raise NotImplementedError("Anthropic API not supported")

                else:
                    print(f"Warning, ")
                    raise Exception(f"Model Provider {Provider} for {Model} not found")
```
